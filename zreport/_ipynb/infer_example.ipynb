{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859fa1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import torch\n",
    "# from models.hgnetv2 import hgnetv2_b5\n",
    "\n",
    "# # load this model\n",
    "# model = hgnetv2_b5(pretrained=False, num_classes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e3b7e3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL argparse.Namespace was not an allowed global by default. Please use `torch.serialization.add_safe_globals([argparse.Namespace])` or the `torch.serialization.safe_globals([argparse.Namespace])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# load the model file\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# load this model\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# state_dict = torch.load(model_file, map_location=\"cuda:0\")\u001b[39;00m\n",
      "File \u001b[1;32me:\\NextCloud\\paper3\\.venv_win\\lib\\site-packages\\torch\\serialization.py:1524\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1516\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1517\u001b[0m                     opened_zipfile,\n\u001b[0;32m   1518\u001b[0m                     map_location,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1521\u001b[0m                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1522\u001b[0m                 )\n\u001b[0;32m   1523\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1524\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1526\u001b[0m             opened_zipfile,\n\u001b[0;32m   1527\u001b[0m             map_location,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1530\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1531\u001b[0m         )\n\u001b[0;32m   1532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL argparse.Namespace was not an allowed global by default. Please use `torch.serialization.add_safe_globals([argparse.Namespace])` or the `torch.serialization.safe_globals([argparse.Namespace])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "from halib import *\n",
    "import torch\n",
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "import timm\n",
    "\n",
    "img = Image.open(\n",
    "    urlopen(\n",
    "        \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png\"\n",
    "    )\n",
    ")\n",
    "\n",
    "model_file = r\"../models/hgnetv2_b5.ssld_stage2_ft_in1k-20250620-50-98.78.pth\"\n",
    "model = timm.create_model(\"hgnetv2_b5.ssld_stage2_ft_in1k\", pretrained=True)\n",
    "model = model.eval()\n",
    "# load the model file\n",
    "state_dict = torch.load(model_file, map_location=\"cuda:0\")\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# load this model\n",
    "# state_dict = torch.load(model_file, map_location=\"cuda:0\")\n",
    "pprint(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcdcf214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'pretrained'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'kwargs'</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[32m'pretrained'\u001b[0m, \u001b[32m'kwargs'\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from timm.models import model_entrypoint\n",
    "model_fn = model_entrypoint(\"hgnetv2_b5.ssld_stage2_ft_in1k\")\n",
    "pprint(model_fn.__code__.co_varnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60312673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.5544</span>,  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.4535</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1.3177</span><span style=\"font-weight: bold\">]])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m-1.5544\u001b[0m,  \u001b[1;36m2.4535\u001b[0m, \u001b[1;36m-1.3177\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m1\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import timm\n",
    "\n",
    "# 모델 경로와 클래스 수 설정\n",
    "MODEL_PATH = \"../models/hgnetv2_b5.ssld_stage2_ft_in1k-20250620-50-98.78.pth\"\n",
    "NUM_CLASSES = 3  # 클래스 수를 실제 모델에 맞게 수정\n",
    "BACKBONE = \"hgnetv2_b5.ssld_stage2_ft_in1k\"\n",
    "\n",
    "# 모델 로드\n",
    "model = timm.create_model(\n",
    "    BACKBONE, pretrained=False, num_classes=NUM_CLASSES, checkpoint_path=MODEL_PATH\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# 이미지 전처리\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((360, 640)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 이미지 불러오기\n",
    "img = Image.open(\n",
    "    r\"E:\\NextCloud\\paper2\\0.datasets\\BOWFire_dataset_chino2015bowfire\\Normal\\not_fire019.png\"\n",
    ").convert(\"RGB\")\n",
    "input_tensor = transform(img).unsqueeze(0)  # 배치 차원 추가\n",
    "\n",
    "# 추론\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "\n",
    "pprint(output)\n",
    "\n",
    "# 결과 출력\n",
    "output = output.argmax(dim=1).item()  # 클래스 인덱스 추출\n",
    "pprint(output)  # 0: fire, 1: none, 2: smoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c08f6e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bright: 10 colors\n",
      "[(0.00784313725490196, 0.24313725490196078, 1.0), (1.0, 0.48627450980392156, 0.0), (0.10196078431372549, 0.788235294117647, 0.2196078431372549), (0.9098039215686274, 0.0, 0.043137254901960784), (0.5450980392156862, 0.16862745098039217, 0.8862745098039215), (0.6235294117647059, 0.2823529411764706, 0.0), (0.9450980392156862, 0.2980392156862745, 0.7568627450980392), (0.6392156862745098, 0.6392156862745098, 0.6392156862745098), (1.0, 0.7686274509803922, 0.0), (0.0, 0.8431372549019608, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "palette_dict = {}\n",
    "\n",
    "for name in sorted(sns.palettes.SEABORN_PALETTES.keys()):\n",
    "    palette_dict[name] = sns.color_palette(name)\n",
    "palette_255 = [(int(r * 255), int(g * 255), int(b * 255)) for r, g, b in palette]\n",
    "\n",
    "# Print one example\n",
    "for name, colors in palette_dict.items():\n",
    "    print(f\"{name}: {len(colors)} colors\")\n",
    "    print(colors)\n",
    "    break  # remove this to print all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_win",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
